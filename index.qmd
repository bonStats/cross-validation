---
title: "Cross-validation"
subtitle: "for statistical modelling"
author: "Dr Joshua Bon"
format: 
  revealjs:
    logo: aulogo.png
    theme: [default, styles.scss]
    chalkboard: true
bibliography: refs.bib
editor: source
---

```{r setup}
#| echo: false

set.seed(20251027)

library(tibble)
library(dplyr)
library(ggplot2)
library(patchwork)
library(polynom)
library(purrr)
library(glue)

```


## Cross-validation

::: {.callout-tip}
## Learning outcomes

1. Describe cross-validation for statistical models
2. Explain purpose of cross-validation 
3. Evaluate suitability of cross-validation types

:::

. . .

::: {.callout-note}
## Prerequisites

- Linear regression

:::

## How do you choose a good model?

```{r}

f <- function(x) 5*x * (x-1) * (x-2)
x <- (1:10)/5

simdata <- tibble(x = x, y = f(x) + rnorm(10))

lms <- map(1:6, \(order) lm(y ~ poly(x, degree = order, raw = TRUE), data = simdata))

aic <- map(lms, \(model) AIC(model))
mse <- map(lms, \(model) mean(residuals(model)^2))

polys_fit <- map(lms, \(model) coef(model) |> as.polynomial() |> as.function())
  
plot_metrics <- map2(polys_fit, 1:length(lms), \(pl,nm) 
  {ggplot(simdata) + ggtitle(glue("poly(degree = {nm})")) + 
      annotate("text", x = 0.25, y = -2.5, label = paste0("MSE == ", round(mse[[nm]], 2)), parse = TRUE) +
      annotate("text", x = 0.25, y = -1.5, label = paste0("AIC == ", round(aic[[nm]], 2)), parse = TRUE) +
    geom_point(aes(x=x,y=y)) + 
    coord_cartesian(ylim = c(-3,3), xlim = c(0,2)) +
    geom_function(fun = pl) +
    theme_bw()}
               )

wrap_plots(plot_metrics, axes='collect')

# plot_minimal <- map2(polys_fit, 1:length(lms), \(pl,nm) 
#   {ggplot(simdata) + 
#     geom_point(aes(x=x,y=y)) + 
#     coord_cartesian(ylim = c(-3,3), xlim = c(0,2)) +
#     geom_function(fun = pl) +
#     theme_minimal() +
#       theme(axis.title = element_blank(), 
#             axis.ticks = element_blank(),
#             axis.text = element_blank())}
#                )


```

::: footer
Mentimeter poll: [https://www.menti.com/alnftv332qkc](https://www.menti.com/alnftv332qkc)
:::


## 

<div style='position: relative; padding-bottom: 56.25%; padding-top: 35px; height: 0; overflow: hidden;'><iframe sandbox='allow-scripts allow-same-origin allow-presentation' allowfullscreen='true' allowtransparency='true' frameborder='0' height='315' src='https://www.mentimeter.com/app/presentation/alrx94gv4qvzxfymuwd3wk1uswrd44ni/embed' style='position: absolute; top: 0; left: 0; width: 100%; height: 100%;' width='420'></iframe></div>


## Model validation

::: {.callout-important}
## Problem

Difficult to assess overfitting with **in-sample validation** metrics

- **In-sample validation**: Assess model with data used in estimation

:::

. . .


::: {.callout-tip}
## Key idea

"Hold-out" some data for validation

- **Out-of-sample validation**: Assess model fit with unused data 

:::




## If we had more data...

```{r}
plots <- map2(polys_fit, 1:length(lms), \(pl,nm) 
  {ggplot(simdata) + ggtitle(glue("poly(degree = {nm})")) +
   geom_point(aes(x=x,y=y), colour = "black") + 
    geom_function(fun = pl) +
    coord_cartesian(ylim = c(-3,3), xlim = c(0,2)) +
    theme_bw()}
               )

wrap_plots(plots, axes='collect')

```
## If we had more data...

```{r}
plots <- map2(polys_fit, 1:length(lms), \(pl,nm) 
  {ggplot(simdata) + ggtitle(glue("poly(degree = {nm})")) +
    geom_function(fun = pl, xlim = range(x)) +
    coord_cartesian(ylim = c(-3,3), xlim = c(0,2)) +
    theme_bw()}
               )

wrap_plots(plots, axes='collect')

```

## If we had more data...

```{r}

newdata <- tibble(x = x, y = f(x) + rnorm(10))

mse_new <- map(lms, \(model) mean((predict(model, newdata = newdata) - newdata$y)^2))

plots <- map2(polys_fit, 1:length(lms), \(pl,nm) 
  {ggplot(newdata) + ggtitle(glue("poly(degree = {nm})")) +
   geom_point(aes(x=x,y=y), colour = "red") + 
    geom_function(fun = pl) +
       #annotate("text", x = 0.25, y = -2.5, label = paste0("RMSE == ", round(mse_new[[nm]], 2)), parse = TRUE) +
    coord_cartesian(ylim = c(-3,3), xlim = c(0,2)) +
    theme_bw()}
               )

wrap_plots(plots, axes='collect')

```

## If we had more data...

```{r}

plots <- map2(polys_fit, 1:length(lms), \(pl,nm) 
  {ggplot(newdata) + ggtitle(glue("poly(degree = {nm})")) +
   geom_point(aes(x=x,y=y), colour = "red") + 
    geom_function(fun = pl) +
       annotate("text", x = 0.25, y = -2.5, label = paste0("MSE == ", round(mse_new[[nm]], 2)), parse = TRUE, colour = "red") +
    coord_cartesian(ylim = c(-3,3), xlim = c(0,2)) +
    theme_bw()}
               )

wrap_plots(plots, axes='collect')

```

## Training and test data

In this example we

1. Used **training data** to fit several statistical models

2. Used [**test data**]{style="color:red;"} to validate those models

_Why?_

. . .

Model performance
: How well does our model generalise to unseen data?

Model selection
: Which model should I choose?


## Train-test data splits

Idea: Split your data into a **training set** and [**test set**]{style="color:red;"}.

```{r}

split <- tibble(row = 1:20, type = "Train-test: 50/50", set = rep(c("1. Training set", "2. Test set"), times = c(10, 10)))

ggplot(split) + 
  geom_point(aes(y = row, x = type, colour = set), shape = 15, size = 5) +
  scale_y_reverse("", n.breaks=20) + 
  scale_x_discrete("") +
  scale_color_manual("", values = c("black","red")) +
  theme_minimal() + 
  theme(panel.grid.minor = element_blank()) +
  guides(colour="none")


```

## Train-test data splits

Idea: Split your data into a **training set** and [**test set**]{style="color:red;"}.

```{r}

split2 <- tibble(row = 1:20, type = "Train-test: 80/20", set = rep(c("1. Training set", "2. Test set"), times = c(16, 4))) |> bind_rows(split)

split3  <- tibble(row = 1:20, type = "Train-test: 90/10", set = rep(c("1. Training set", "2. Test set"), times = c(18, 2))) |> bind_rows(split2)

plot_split3 <- ggplot(split3) + 
  geom_point(aes(y = row, x = type, colour = set), shape = 15, size = 5) +
  scale_y_reverse("", n.breaks=20) + 
  scale_x_discrete("") +
  scale_color_manual("", values = c("black","red")) +
  theme_minimal() + 
  theme(panel.grid.minor = element_blank()) +
  guides(colour="none")

plot_split3

```

## Train-test data splits

:::: {.columns}

::: {.column width="60%" .incremental}


What are the trade-offs?

::: {.fragment .fade-in-then-out}
$\uparrow$ **training** set
:::

::: {.fragment .fade-in}
$\uparrow$ [**test set**]{style="color:red;"}
:::

:::

::: {.column width="40%"}

```{r}
#| fig-width: 4
#| fig-height: 6
#| message: false
plot_split3 + theme(axis.text.y = element_blank())
```

:::

::::


## Train-test data splits

:::: {.columns}

::: {.column width="60%" .incremental}

Considerations

- Limits from small sample sizes
- Purpose of validation
    - Prediction
- Test set selection
    - Random
    - Ordered/grouped data

:::

::: {.column width="40%"}

```{r}
#| fig-width: 4
#| fig-height: 6
#| message: false
plot_split3 + theme(axis.text.y = element_blank())
```

:::

::::

# How do we best utilise all the data for model validation?

## k-fold cross-validation

Duplicate data into $k$ "folds": **training sets** and [**test sets**]{style="color:red;"}

```{r}

build_folds <- function(n,k){
  step_val <- n/k
  c1 <- seq(from = 0, to = n-step_val, by = step_val)
  c2 <- c(rep(step_val, times = k-1), 0)
  c3 <- rev(c1)
  c4 <- c(rep(0, times = k-1), step_val)
  m <- matrix(c(c1,c2,c3,c4), ncol = 4)
  
  apply(m, MARGIN = 1, 
        \(x) rep(c("1. Training set", "2. Test set", "1. Training set", "2. Test set"), times = x),
        simplify = FALSE
        )
}

kfold <- build_folds(20,5) |> 
  map(\(x) tibble(row = 1:20, set = x, type = "Train-test: 80/20")) |>
  bind_rows(.id = "fold") |>
  mutate(fold = paste("Fold", fold))


ggplot(kfold) + 
  geom_point(aes(y = row, x = type, colour = set), shape = 15, size = 5) +
  scale_y_reverse("", n.breaks=20) + 
  scale_x_discrete("") +
  scale_color_manual("", values = c("black","red")) +
  theme_bw() + 
  theme(panel.grid.minor = element_blank(), axis.text.y = element_blank(), axis.ticks.y = element_blank()) +
  guides(colour="none") + 
  facet_grid(rows = ~fold)


```

## Leave-one-out cross-validation $k=n$

```{r}

kfold <- build_folds(20,20) |> 
  map(\(x) tibble(row = 1:20, set = x, type = "LOOCV")) |>
  bind_rows(.id = "id") |>
  mutate(fold = paste("Fold", id)) |>
  mutate(fold = ordered(fold, levels = unique(paste("Fold", id))))


ggplot(kfold) + 
  geom_point(aes(y = row, x = type, colour = set), shape = 15, size = 3) +
  scale_y_reverse("", n.breaks=20) + 
  scale_x_discrete("") +
  scale_color_manual("", values = c("black","red")) +
  theme_bw() + 
  theme(panel.grid.minor = element_blank(), axis.text.y = element_blank(), axis.ticks.y = element_blank()) +
  guides(colour="none") + 
  facet_wrap(~fold, nrow=2)


```


## Cross-validation workflow

```{mermaid}
flowchart LR
  Data[Dataset] --> Fold1(Fold 1)
  Data --> Fold2(Fold 2)
  Data --> Fold3(Fold 3)
  Data --> Fold4(Fold 4)
  Data --> Fold5(Fold 5)
  
  Fold1 --> Train1(Train models)
  Fold2 --> Train2(Train models)
  Fold3 --> Train3(Train models)
  Fold4 --> Train4(Train models)
  Fold5 --> Train5(Train models)
  
  Train1 --> Test1(Test models)
  Train2 --> Test2(Test models)
  Train3 --> Test3(Test models)
  Train4 --> Test4(Test models)
  Train5 --> Test5(Test models)
  
  Test1 --> Sum(Summarise)
  Test2 --> Sum
  Test3 --> Sum
  Test4 --> Sum
  Test5 --> Sum
  
  Sum --> Eval(Evaluate/select model)
```


## Summary

:::{.incremental}

CV $\approx$ out-of-sample validation using **all** the data

- Model agnostic

- Used for model selection
    - hyper-parameter tuning for machine learning methods

- Complements your data science toolbox
    - does not replace assumption checking

- Many extensions
    - CV for grouped data, time series data


:::

